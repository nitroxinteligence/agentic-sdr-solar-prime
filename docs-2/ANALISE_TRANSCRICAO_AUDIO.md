# An√°lise da Transcri√ß√£o de √Åudio e Interpreta√ß√£o pelo Agente

## Diagn√≥stico do Fluxo Atual

A an√°lise do c√≥digo-fonte (`app/api/webhooks.py`, `app/agents/agentic_sdr.py`, `app/services/audio_transcriber.py`) revela o seguinte fluxo para o processamento de mensagens de √°udio:

1.  **Recebimento da Mensagem de √Åudio (`app/api/webhooks.py`)**:
    *   Quando uma mensagem de √°udio √© recebida via webhook da Evolution API, a fun√ß√£o `process_new_message` √© acionada.
    *   Dentro de `process_message_with_agent`, o √°udio √© baixado e codificado em Base64.
    *   Um dicion√°rio `media_data` √© criado, contendo o tipo (`"audio"`), o `mimetype`, e os dados do √°udio em Base64 (`data`).
    *   A fun√ß√£o `agentic.process_message()` √© ent√£o chamada, passando o `media_data` como um dos argumentos.

2.  **Processamento Multimodal pelo `AgenticSDR` (`app/agents/agentic_sdr.py`)**:
    *   O m√©todo `process_multimodal_content(self, media_type, media_data, ...)` √© invocado.
    *   Se o `media_type` for `"audio"`, o sistema verifica se a transcri√ß√£o de voz est√° habilitada (`settings.enable_voice_message_transcription`).
    *   Em seguida, `audio_transcriber.transcribe_from_base64()` √© chamado com os dados do √°udio em Base64.
    *   **Ponto Crucial**: Se a transcri√ß√£o for bem-sucedida (`result["status"] == "success"`), o texto transcrito √© extra√≠do (`transcribed_text = result["text"]`).
    *   O m√©todo `process_multimodal_content` retorna um dicion√°rio que **inclui** o texto transcrito sob a chave `"transcription"`.

3.  **Servi√ßo de Transcri√ß√£o de √Åudio (`app/services/audio_transcriber.py`)**:
    *   Este servi√ßo √© respons√°vel por decodificar o √°udio, convert√™-lo para um formato process√°vel (se necess√°rio, usando `ffmpeg` para √°udios do WhatsApp como Opus) e, finalmente, transcrev√™-lo para texto usando `SpeechRecognition` (com fallback para OpenAI Whisper).
    *   A sa√≠da √© um dicion√°rio contendo o texto transcrito na chave `"text"`.

## Conclus√£o: A Transcri√ß√£o Chega ao Agente?

**Sim, a transcri√ß√£o do √°udio est√° sendo realizada e o texto transcrito √© passado para o agente.**

No entanto, h√° um detalhe importante:

*   O m√©todo `extract_message_content` (em `app/api/webhooks.py`) √© respons√°vel por extrair o conte√∫do da mensagem principal. Para √°udios, ele retorna uma string gen√©rica como `"[Nota de voz recebida]"` ou `"[√Åudio recebido]"`.
*   Quando `AgenticSDR.process_message` chama `self.team.arun()` (ou `self.agent.run()`), o `message` principal que o agente recebe no `team_prompt` (ou nas `instructions` do agente) √© essa string gen√©rica.
*   O texto transcrito do √°udio est√° dispon√≠vel para o agente, mas dentro do dicion√°rio `multimodal_result` (que √© parte do `context` passado para o agente).

**Isso significa que o agente tem acesso ao texto transcrito, mas o prompt principal que guia sua interpreta√ß√£o inicial pode n√£o estar explicitamente direcionado para usar essa transcri√ß√£o como o conte√∫do prim√°rio da mensagem do usu√°rio.**

## Recomenda√ß√£o para Melhoria

Para garantir que o agente interprete e formule a resposta com base no conte√∫do *transcrito* do √°udio de forma otimizada, √© fundamental ajustar o prompt do agente (`app/prompts/prompt-agente.md`).

O prompt deve ser instru√≠do a:

1.  **Reconhecer a presen√ßa de `multimodal_result` no contexto.**
2.  **Priorizar o `multimodal_result['transcription']` como o conte√∫do principal da mensagem do usu√°rio** sempre que `media_type` for `"audio"` e a transcri√ß√£o estiver dispon√≠vel.

**Exemplo de como a instru√ß√£o no prompt poderia ser adaptada (conceitual):**

```
üéµ SE FOR √ÅUDIO:
- VOC√ä DEVE UTILIZAR O TEXTO DA TRANSCRI√á√ÉO DISPON√çVEL NO CONTEXTO PARA ENTENDER A MENSAGEM DO USU√ÅRIO.
- RESPONDA ao conte√∫do do √°udio de forma natural.
- SE MENCIONAREM valores de conta, processe como qualifica√ß√£o.
- SE PEDIREM informa√ß√µes, forne√ßa de forma clara.
- MANTENHA a conversa fluida e natural.
- TRATE como se estivesse respondendo a um √°udio mesmo.
```
