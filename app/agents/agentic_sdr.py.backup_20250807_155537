"""
AGENTIC SDR - Agente Principal Conversacional Ultra-Humanizado
Com análise contextual inteligente das últimas 100 mensagens
"""

import asyncio
import json
import random
import re
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from enum import Enum
import base64

from agno.agent import Agent
from agno.models.google import Gemini
# OpenAI via requests - contorna problemas do SDK
try:
    import requests
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

from app.utils.time_utils import get_period_of_day


class SimpleOpenAIWrapper:
    """
    Wrapper OpenAI usando requests diretos para evitar problemas do SDK
    """
    def __init__(self, api_key, id="o3-mini", max_tokens=4000, temperature=0.7):
        if not OPENAI_AVAILABLE:
            raise ImportError("requests não disponível")
            
        import requests
        self.api_key = api_key
        self.model_id = id
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.base_url = "https://api.openai.com/v1"
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        })
    
    def run(self, message, **kwargs):
        """Interface compatível com AGNO usando requests diretos"""
        try:
            payload = {
                "model": self.model_id,
                "messages": [{"role": "user", "content": str(message)}],
                "max_completion_tokens": self.max_tokens  # o3-mini usa max_completion_tokens, sem temperature
            }
            
            response = self.session.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"]
            else:
                raise Exception(f"API Error {response.status_code}: {response.text}")
                
        except Exception as e:
            raise Exception(f"OpenAI o3-mini falhou: {e}")
    
    def __str__(self):
        return f"SimpleOpenAI({self.model_id})"
from agno.memory import AgentMemory
from agno.knowledge import AgentKnowledge
from agno.tools import tool
from loguru import logger
from app.utils.logger import emoji_logger
from app.utils.optional_storage import OptionalStorage
from app.utils.agno_media_detection import agno_media_detector
from app.utils.retry_handler import async_retry, GEMINI_RETRY_CONFIG, OPENAI_RETRY_CONFIG

from app.config import settings
from app.integrations.supabase_client import supabase_client
from app.teams.sdr_team import SDRTeam

# Serviços REMOVIDOS - substituídos por funções simples inline
# from app.services.agno_context_agent import format_context_with_agno
# from app.services.document_processor_enhanced import process_document_enhanced
# KnowledgeService - Substitui KnowledgeAgent com implementação mais simples
from app.services.knowledge_service import knowledge_service


class IntelligentModelFallback:
    """
    Wrapper inteligente para gerenciar fallback automático entre modelos
    Detecta erros Gemini e automaticamente usa OpenAI o3-mini
    """
    
    def __init__(self, settings):
        self.settings = settings
        self.primary_model = None
        self.fallback_model = None
        self.current_model = None
        # Importar o detector de mídia como atributo da classe
        self.agno_media_detector = agno_media_detector
        self.fallback_active = False
        
        # Configurações de retry para Gemini
        self.max_retry_attempts = getattr(settings, 'gemini_retry_attempts', 2)  # 2 tentativas de retry
        self.retry_delay = getattr(settings, 'gemini_retry_delay', 5.0)  # 5 segundos entre tentativas
        
        # Armazenar instructions do agente principal
        self._agent_instructions = None
        
        self._initialize_models()
    
    def set_agent_instructions(self, instructions):
        """Define as instructions do agente principal para usar nos temp_agents"""
        self._agent_instructions = instructions
    
    @property
    def id(self):
        """Expõe o ID do modelo atual para compatibilidade com agno.agent.Agent"""
        if self.current_model:
            return self.current_model.id
        return "unknown_model"
    
    @property
    def provider(self):
        """Expõe o provider do modelo atual para compatibilidade com agno.agent.Agent"""
        if self.current_model and hasattr(self.current_model, 'provider'):
            return self.current_model.provider
        # Inferir provider do ID do modelo
        if self.current_model and hasattr(self.current_model, 'id'):
            model_id = self.current_model.id
            if 'gemini' in model_id.lower():
                return 'google'
            elif 'gpt' in model_id.lower() or 'o3' in model_id.lower():
                return 'openai'
        return "unknown"
    
    def __getattr__(self, name):
        """
        Delega qualquer atributo/método não encontrado para o modelo atual.
        Isso garante compatibilidade total com agno.agent.Agent.
        """
        if name.startswith('_'):  # Evitar recursão infinita com atributos privados
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")
        
        if self.current_model is None:
            raise AttributeError(f"No current model available to delegate '{name}'")
        
        # Tentar obter o atributo/método do modelo atual
        try:
            return getattr(self.current_model, name)
        except AttributeError:
            # Se o modelo atual não tem o atributo, retornar um valor padrão ou função vazia
            if name == 'get_instructions_for_model':
                # Retornar função vazia que retorna string vazia
                return lambda: ""
            raise AttributeError(f"'{self.__class__.__name__}' object and current model have no attribute '{name}'")
    
    def _initialize_models(self):
        """Inicializa os modelos primário e fallback"""
        try:
            # Modelo primário (Gemini)
            if "gemini" in self.settings.primary_ai_model.lower():
                self.primary_model = Gemini(
                    id=self.settings.primary_ai_model,
                    api_key=self.settings.google_api_key,
                    temperature=self.settings.ai_temperature,
                    max_output_tokens=self.settings.ai_max_tokens
                )
                emoji_logger.system_ready("Modelo primário Gemini configurado", 
                                         model=self.settings.primary_ai_model)
            
            # Modelo fallback (OpenAI o3-mini)  
            if self.settings.enable_model_fallback and self.settings.openai_api_key and OPENAI_AVAILABLE:
                try:
                    # Usar wrapper customizado OpenAI (compatível)
                    self.fallback_model = SimpleOpenAIWrapper(
                        api_key=self.settings.openai_api_key,
                        id="o3-mini",
                        max_tokens=self.settings.ai_max_tokens,
                        temperature=self.settings.ai_temperature
                    )
                    emoji_logger.system_ready("Modelo fallback OpenAI o3-mini configurado")
                        
                except Exception as e:
                    emoji_logger.system_warning(f"Falha ao configurar OpenAI o3-mini fallback: {e}")
                    self.fallback_model = None
            else:
                if not OPENAI_AVAILABLE:
                    emoji_logger.system_warning("OpenAI não disponível - fallback desabilitado")
                elif not self.settings.openai_api_key:
                    emoji_logger.system_warning("OPENAI_API_KEY não configurada - fallback desabilitado")
                else:
                    emoji_logger.system_warning("Fallback desabilitado por configuração")
                self.fallback_model = None
            
            # Define modelo atual
            self.current_model = self.primary_model
            
        except Exception as e:
            emoji_logger.system_error("Model Init", f"Erro na inicialização de modelos: {e}")
            raise
    
    def _is_gemini_error(self, error) -> bool:
        """Detecta se é um erro que requer fallback"""
        error_str = str(error).lower()
        
        # Erros que requerem fallback
        fallback_triggers = [
            "500 internal",
            "503 service unavailable", 
            "502 bad gateway",
            "timeout",
            "connection error",
            "server error",
            "internal error has occurred"
        ]
        
        return any(trigger in error_str for trigger in fallback_triggers)
    
    def _should_retry_with_fallback(self, error) -> bool:
        """Determina se deve tentar fallback"""
        return (
            not self.fallback_active and  # Não estamos já usando fallback
            self.fallback_model is not None and  # Temos modelo fallback
            self._is_gemini_error(error)  # É um erro que requer fallback
        )
    
    @async_retry(GEMINI_RETRY_CONFIG)
    async def _gemini_call_with_retry(self, message: str, **kwargs):
        """Chamada Gemini com retry automático via decorador"""
        if self.primary_model:
            # SOLUÇÃO DEFINITIVA: Usar arun() para async ou asyncio.to_thread para sync
            from agno.agent import Agent
            import asyncio
            
            # Usar instructions do agente principal se disponível, senão usar padrão
            instructions = self._agent_instructions or kwargs.pop('instructions', 'Você é Helen, uma vendedora especializada em energia solar da SolarPrime. Responda de forma natural, empática e focada em ajudar o cliente.')
            
            temp_agent = Agent(
                model=self.primary_model,
                markdown=True,
                show_tool_calls=False,
                instructions=instructions
            )
            
            # Verificar se tem arun (async) ou usar run em thread
            if hasattr(temp_agent, 'arun'):
                response = await temp_agent.arun(message, **kwargs)
            else:
                # Executar run() síncrono em thread para não bloquear
                response = await asyncio.to_thread(temp_agent.run, message, **kwargs)
            
            return response
            
        raise Exception("Modelo primário Gemini não disponível")
    
    @async_retry(OPENAI_RETRY_CONFIG)
    async def _openai_call_with_retry(self, message: str, **kwargs):
        """Chamada OpenAI com retry automático via decorador"""
        if self.fallback_model:
            # OpenAI wrapper tem run() implementado
            return self.fallback_model.run(message, **kwargs)
        raise Exception("Modelo fallback OpenAI não disponível")
    
    async def _retry_with_backoff(self, message: str, **kwargs):
        """
        Tenta executar o Gemini com retry e backoff
        Retorna a resposta ou None se todas as tentativas falharem
        """
        import asyncio
        from agno.agent import Agent
        
        last_error = None
        
        for attempt in range(self.max_retry_attempts):
            try:
                emoji_logger.system_info(f"🔄 Retry Gemini - Tentativa {attempt + 1}/{self.max_retry_attempts}")
                
                # O Gemini no AGNO precisa ser usado através de um Agent
                # Usar instructions do agente principal se disponível, senão usar padrão
                instructions = self._agent_instructions or kwargs.pop('instructions', 'Você é Helen, uma vendedora especializada em energia solar da SolarPrime. Responda de forma natural, empática e focada em ajudar o cliente.')
                
                temp_agent = Agent(
                    model=self.primary_model,
                    markdown=True,
                    show_tool_calls=False,
                    instructions=instructions
                )
                
                # Usar arun se disponível, senão run em thread
                if hasattr(temp_agent, 'arun'):
                    response = await temp_agent.arun(message, **kwargs)
                else:
                    response = await asyncio.to_thread(temp_agent.run, message, **kwargs)
                
                if attempt > 0:
                    emoji_logger.system_ready(f"✅ Gemini recuperado após {attempt + 1} tentativa(s)")
                
                return response
                
            except Exception as e:
                last_error = e
                error_str = str(e).lower()
                
                # Só faz retry se for erro temporário do Gemini
                if self._is_gemini_error(e):
                    if attempt < self.max_retry_attempts - 1:
                        emoji_logger.system_warning(
                            f"⚠️ Erro Gemini: {e}. Aguardando {self.retry_delay}s antes da próxima tentativa..."
                        )
                        await asyncio.sleep(self.retry_delay)
                    else:
                        emoji_logger.system_warning(
                            f"❌ Gemini falhou após {self.max_retry_attempts} tentativas"
                        )
                else:
                    # Se não for erro temporário, não faz retry
                    emoji_logger.system_warning(f"❌ Erro Gemini não recuperável: {e}")
                    raise e
        
        # Se chegou aqui, todas as tentativas falharam
        return None
    
    async def run(self, message: str, **kwargs):
        """
        Executa o modelo com retry inteligente e fallback
        Fluxo: Gemini → Retry (se erro 500/503) → Fallback OpenAI (se retry falhar)
        """
        # Se já estamos usando fallback, usa direto com retry
        if self.fallback_active and self.current_model == self.fallback_model:
            try:
                response = await self._openai_call_with_retry(message, **kwargs)
                emoji_logger.system_info("📍 Usando fallback OpenAI o3-mini com retry")
                return response
            except Exception as e:
                emoji_logger.system_error("Fallback OpenAI falhou após múltiplas tentativas", error=str(e))
                raise e
        
        # Tenta com modelo primário (Gemini) usando retry automático
        try:
            if self.primary_model:
                response = await self._gemini_call_with_retry(message, **kwargs)
                
                # Se estava usando fallback e Gemini funcionou, desativa fallback
                if self.fallback_active:
                    emoji_logger.system_ready("✅ Gemini recuperado, desativando fallback")
                    self.fallback_active = False
                    self.current_model = self.primary_model
                
                return response
                
        except Exception as e:
            emoji_logger.system_warning(f"⚠️ Gemini falhou após múltiplas tentativas: {e}")
            
            # Se temos fallback, ativa OpenAI com retry
            if self.fallback_model is not None:
                emoji_logger.system_warning("🔄 Ativando fallback OpenAI o3-mini com retry...")
                
                try:
                    self.current_model = self.fallback_model
                    self.fallback_active = True
                    
                    response = await self._openai_call_with_retry(message, **kwargs)
                    emoji_logger.system_ready("✅ Fallback OpenAI o3-mini ativado com sucesso")
                    return response
                        
                except Exception as fallback_error:
                    emoji_logger.system_error("Fallback OpenAI também falhou", error=str(fallback_error))
                    # Volta para modelo primário para próxima tentativa
                    self.current_model = self.primary_model
                    self.fallback_active = False
                    raise fallback_error
            else:
                # Erro não recuperável, não faz retry
                emoji_logger.system_error("Erro não recuperável no Gemini", error=str(e))
                raise e
    
    def reset_to_primary(self):
        """Força volta ao modelo primário"""
        if self.primary_model:
            self.current_model = self.primary_model
            self.fallback_active = False
            emoji_logger.system_info("Modelo resetado para primário (Gemini)")
    
    def get_current_model_info(self) -> dict:
        """Retorna informações do modelo atual"""
        return {
            "current_model": self.current_model.__class__.__name__ if self.current_model else None,
            "fallback_active": self.fallback_active,
            "has_fallback": self.fallback_model is not None
        }
    
    # Alias para compatibilidade com AGNO Agent
    async def arun(self, message: str, **kwargs):
        """Alias para run() - mantém compatibilidade com AGNO Agent que espera arun()"""
        return await self.run(message, **kwargs)


class ConversationContext(Enum):
    """Contextos de conversa detectados"""
    INITIAL_CONTACT = "initial_contact"
    QUALIFICATION_NEEDED = "qualification_needed"
    HIGH_VALUE_LEAD = "high_value_lead"
    SCHEDULING_READY = "scheduling_ready"
    OBJECTION_HANDLING = "objection_handling"
    TECHNICAL_QUESTIONS = "technical_questions"
    FOLLOW_UP_REQUIRED = "follow_up_required"
    COMPLEX_NEGOTIATION = "complex_negotiation"
    DOCUMENT_ANALYSIS = "document_analysis"
    CRM_UPDATE_NEEDED = "crm_update_needed"


class EmotionalState(Enum):
    """Estados emocionais do AGENTIC SDR - Alinhados com banco de dados"""
    ENTUSIASMADA = "ENTUSIASMADA"
    CURIOSA = "CURIOSA"
    CONFIANTE = "CONFIANTE"
    DUVIDOSA = "DUVIDOSA"
    NEUTRA = "NEUTRA"


class AgenticSDR:
    """
    Agente Principal AGENTIC SDR Ultra-Humanizado
    
    Características:
    - Análise contextual inteligente das últimas 100 mensagens
    - Personalidade ultra-humanizada com estados emocionais
    - Multimodal (imagens, áudio, documentos)
    - Reasoning para casos complexos
    - Memory persistente com pgvector
    - Decision engine inteligente para SDR Team
    """
    
    def __init__(self):
        """Inicializa o AGENTIC SDR com todas as capacidades"""
        self.is_initialized = False
        
        # Importar o detector de mídia como atributo da classe
        self.agno_media_detector = agno_media_detector
        
        # Armazenar referência ao settings
        self.settings = settings
        
        # Configurações de funcionalidades baseadas no .env
        self.context_analysis_enabled = settings.enable_context_analysis
        self.reasoning_enabled = settings.agno_reasoning_enabled
        self.multimodal_enabled = settings.enable_multimodal_analysis
        self.knowledge_search_enabled = settings.enable_knowledge_base
        
        # Cache removido - sempre buscar histórico atualizado do Supabase
        self.sentiment_analysis_enabled = settings.enable_sentiment_analysis
        self.emotional_triggers_enabled = settings.enable_emotional_triggers
        self.lead_scoring_enabled = settings.enable_lead_scoring
        self.emoji_usage_enabled = settings.enable_emoji_usage
        
        # REMOVIDO: Estado emocional como atributo de instância
        # Agora o estado é gerenciado por conversa no banco de dados
        
        # Configuração do PostgreSQL/Supabase para storage com fallback
        # Storage persistente com fallback para memória se PostgreSQL não disponível
        self.storage = OptionalStorage(
            table_name="agentic_sdr_sessions",  # Nome da tabela para sessões do agente
            db_url=settings.get_postgres_url(),  # URL já inclui autenticação
            schema="public",  # Schema do Supabase
            auto_upgrade_schema=True  # Auto-atualiza schema se necessário
        )
        
        # Setup models BEFORE Memory (needed for fallback)
        self._setup_models()
        
        # Memory v2 - SOLUÇÃO DEFINITIVA (conforme ANALISE_ERRO_AGENTMEMORY.md)
        # AgentMemory agora é apenas para memória de trabalho (RAM), sem db
        # O storage é passado diretamente para o Agent, não para AgentMemory
        try:
            # CORREÇÃO: AgentMemory sem parâmetro db (arquitetura nova do AGNO)
            self.memory = AgentMemory(
                create_user_memories=True,
                create_session_summary=True
            )
            emoji_logger.system_ready("Memory", status="configurada (in-memory)")
        except Exception as e:
            emoji_logger.system_info(f"Memory fallback: {str(e)[:100]}...")
            # Se AgentMemory falhar completamente, usar None
            # O Agent da AGNO aceita memory=None
            self.memory = None
            emoji_logger.system_info("💾 Memory: Desabilitado (Agent funcionará sem memória)")
        
        # Knowledge base SEM PostgreSQL - usando apenas dados locais
        # Sistema funciona perfeitamente sem vector database PostgreSQL
        try:
            # AgentKnowledge sem vector_db (usa conhecimento local)
            self.knowledge = AgentKnowledge(
                num_documents=10  # Busca em conhecimento local/memória
            )
            self.vector_db = None  # Não precisamos de PostgreSQL
            emoji_logger.system_ready("Knowledge", status="local ativo")
        except Exception as e:
            emoji_logger.system_info(f"Knowledge desabilitado: {str(e)[:40]}...")
            self.vector_db = None
            self.knowledge = None
        
        # SDR Team para tarefas especializadas
        self.sdr_team = None
        
        # Context analyzer tools - apenas habilitar as ferramentas configuradas
        self.tools = []
        
        # Adicionar ferramentas baseadas nas configurações
        if self.context_analysis_enabled:
            self.tools.append(self.analyze_conversation_context)
            self.tools.append(self.get_last_100_messages)
        
        if self.emotional_triggers_enabled:
            self.tools.append(self.detect_emotional_triggers)
        
        # Sempre incluir decisão do SDR Team
        self.tools.append(self.should_call_sdr_team)
        
        if self.multimodal_enabled:
            self.tools.append(self.process_multimodal_content)
        
        if self.knowledge_search_enabled:
            self.tools.append(self.search_knowledge_base)
            self.tools.append(self.analyze_energy_bill)
        
        tool_names = []
        for t in self.tools:
            if hasattr(t, '__name__'):
                tool_names.append(t.__name__)
            elif hasattr(t, 'name'):
                tool_names.append(t.name)
            else:
                tool_names.append(str(t))
        emoji_logger.agentic_thinking(f"Tools habilitadas: {len(self.tools)}", 
                                      tools=tool_names)
        
        # Criar o agente principal
        self._create_agentic_agent()
        
        emoji_logger.agentic_start("Sistema inicializado com sucesso", 
                                   context_enabled=self.context_analysis_enabled,
                                   reasoning_enabled=self.reasoning_enabled,
                                   multimodal_enabled=self.multimodal_enabled)
    
    def _setup_models(self):
        """Configura modelos com fallback inteligente para OpenAI o3-mini"""
        try:
            # Usar novo sistema de fallback inteligente
            self.intelligent_model = IntelligentModelFallback(settings)
            
            # Para compatibilidade, manter referência self.model
            self.model = self.intelligent_model.current_model
            
            # Modelo de reasoning - Gemini 2.0 Flash Thinking (se habilitado)
            if self.reasoning_enabled and settings.google_api_key:
                try:
                    self.reasoning_model = Gemini(
                        id="gemini-2.0-flash-thinking-exp-01-21",
                        api_key=settings.google_api_key,
                        thinking_budget=8192,
                        include_thoughts=False  # ✅ CORRIGIDO: Não vazar raciocínio interno para usuário
                    )
                    emoji_logger.system_ready("Modelo reasoning configurado", model="gemini-2.0-flash-thinking")
                except Exception as e:
                    emoji_logger.system_warning(f"Reasoning model falhou, usando modelo principal: {e}")
                    self.reasoning_model = self.intelligent_model.current_model
            else:
                self.reasoning_model = self.intelligent_model.current_model
            
            # Log status final
            model_info = self.intelligent_model.get_current_model_info()
            emoji_logger.system_ready("Sistema de modelos configurado", 
                                     primary_model=settings.primary_ai_model,
                                     fallback_available=model_info['has_fallback'],
                                     reasoning_enabled=self.reasoning_enabled)
                
        except Exception as e:
            emoji_logger.system_error("Model Config", f"Erro crítico na configuração de modelos: {e}")
            raise
    
    def _create_agentic_agent(self):
        """Cria o agente AGENTIC SDR com personalidade completa"""
        
        # Carregar prompt completo do AGENTIC SDR
        with open("app/prompts/prompt-agente.md", "r", encoding="utf-8") as f:
            agentic_prompt = f.read()
        
        # Adicionar instruções de análise contextual
        enhanced_prompt = agentic_prompt + """

## 🧠 ANÁLISE CONTEXTUAL INTELIGENTE

Você SEMPRE deve:
1. Buscar e analisar as últimas 100 mensagens da conversa
2. Entender o contexto completo antes de responder
3. Detectar padrões e necessidades não explícitas
4. Decidir inteligentemente quando acionar especialistas

### Sistema de Decisão Contextual
Analise TODOS os seguintes fatores antes de decidir:
- Histórico completo da conversa
- Estágio atual do lead no funil
- Complexidade da solicitação
- Necessidade de expertise especializada
- Urgência e prioridade
- Estado emocional do lead

### Quando Acionar SDR Team
APENAS quando detectar necessidade REAL de:
- Qualificação técnica avançada
- Agendamento com múltiplas validações
- Análise de conta de luz com cálculos
- Follow-up estratégico complexo
- Atualização crítica no CRM
- Conhecimento técnico muito específico

LEMBRE-SE: Você resolve 90% das conversas sozinha!
"""
        
        self.agent = Agent(
            name="AGENTIC SDR",
            model=self.intelligent_model,  # CORREÇÃO: Passar o wrapper, não o modelo direto
            instructions=enhanced_prompt,
            tools=self.tools,
            storage=self.storage,  # CORREÇÃO: Passar storage diretamente para o Agent
            memory=self.memory,    # Passar a memória simples (ou None se falhou)
            knowledge=self.knowledge,
            show_tool_calls=False,
            markdown=True,
            debug_mode=settings.debug,
            # Context includes personality configurations
            context={
                "emotional_state": "ENTUSIASMADA",  # Estado padrão, será sobrescrito em process_message
                "cognitive_load": 0.0,
                "current_time": datetime.now().strftime("%H:%M"),
                "day_of_week": datetime.now().strftime("%A"),
                "period_of_day": get_period_of_day(settings.timezone)  # Manhã, Tarde ou Noite
            }
        )
        
        # Configurar as instructions no modelo wrapper para uso em temp_agents
        if hasattr(self.intelligent_model, 'set_agent_instructions'):
            self.intelligent_model.set_agent_instructions(enhanced_prompt)
    
    def _is_complex_message(self, message: str) -> bool:
        """
        Determina se a mensagem é complexa e requer reasoning (OTIMIZADO)
        
        Critérios mais restritivos para economizar tempo:
        - Mensagens muito curtas (< 15 chars) = simples
        - Saudações e respostas diretas = simples
        - Apenas perguntas ELABORADAS = complexa
        """
        message_lower = message.lower().strip()
        
        # Mensagens muito curtas são sempre simples - AUMENTADO DE 10 PARA 15
        if len(message_lower) < 15:
            return False
            
        # Respostas diretas que NÃO precisam reasoning - EXPANDIDO
        simple_responses = {
            'oi', 'olá', 'bom dia', 'boa tarde', 'boa noite',
            'tudo bem', 'tudo certo', 'sim', 'não', 'ok', 'certo', 
            'beleza', 'entendi', 'pode ser', 'claro', 'com certeza',
            'obrigado', 'obrigada', 'tchau', 'até mais', 'valeu',
            'ta bom', 'ta ok', 'legal', 'ótimo', 'perfeito',
            'isso', 'isso mesmo', 'exato', 'concordo'
        }
        
        # Se é uma resposta simples, não precisa reasoning
        if message_lower in simple_responses or any(
            message_lower.startswith(resp) and len(message_lower) < 25 
            for resp in simple_responses
        ):
            return False
        
        # SÓ ativar reasoning para questões REALMENTE complexas
        complex_indicators = [
            'como funciona', 'me explica', 'não entendi',
            'quanto custa', 'qual o valor', 'economia',
            'comparar', 'diferença', 'vantagem',
            'garantia', 'manutenção', 'instalação',
            'o que é', 'por que', 'quando'
        ]
        
        # Precisa ter pelo menos 2 indicadores OU pergunta muito elaborada
        indicator_count = sum(1 for ind in complex_indicators if ind in message_lower)
        has_multiple_questions = message.count('?') > 1
        is_long_question = '?' in message and len(message) > 50
        
        return indicator_count >= 2 or has_multiple_questions or is_long_question
    
    async def analyze_conversation_context(
        self,
        phone: str,
        current_message: str
    ) -> Dict[str, Any]:
        """
        Analisa contexto completo da conversa
        
        Args:
            phone: Número do telefone
            current_message: Mensagem atual
            
        Returns:
            Análise contextual completa
        """
        try:
            # Buscar últimas 100 mensagens
            messages = await self.get_last_100_messages(phone)
            
            # Análise de padrões
            context_analysis = {
                "message_count": len(messages),
                "conversation_duration": self._calculate_duration(messages),
                "lead_engagement_level": self._analyze_engagement(messages),
                "detected_intents": self._detect_intents(messages),
                "emotional_trajectory": self._analyze_emotional_trajectory(messages),
                "key_topics": self._extract_key_topics(messages),
                "qualification_signals": self._detect_qualification_signals(messages),
                "objections_raised": self._extract_objections(messages),
                "decision_stage": self._determine_decision_stage(messages),
                "urgency_level": self._assess_urgency(messages, current_message)
            }
            
            # Determinar contexto principal
            context_analysis["primary_context"] = self._determine_primary_context(
                context_analysis
            )
            
            # Recomendação de ação
            context_analysis["recommended_action"] = self._recommend_action(
                context_analysis
            )
            
            emoji_logger.agentic_context(f"Contexto identificado: {context_analysis['primary_context']}",
                                        messages_analyzed=len(messages),
                                        context_type=context_analysis['primary_context'])
            
            return context_analysis
            
        except Exception as e:
            emoji_logger.system_error("AGENTIC SDR", f"Erro na análise contextual: {e}")
            return {
                "primary_context": ConversationContext.INITIAL_CONTACT.value,
                "error": str(e)
            }
    
    async def get_last_100_messages(self, identifier: str) -> List[Dict[str, Any]]:
        """
        Busca as últimas 100 mensagens do Supabase (sempre atualizado)
        
        Args:
            identifier: Número do telefone ou conversation_id
            
        Returns:
            Lista com últimas 100 mensagens
        """
        
        # LOG CRÍTICO: Rastrear todas as chamadas
        emoji_logger.system_info(f"🔍 HISTÓRICO: Buscando mensagens para identifier={identifier}")
        
        # Validação de entrada
        if not identifier:
            emoji_logger.system_error("HISTÓRICO", "❌ Identifier vazio ou None!")
            return []
        
        try:
            # VALIDAÇÃO: Verificar se identifier é válido
            if not identifier:
                emoji_logger.system_warning("get_last_100_messages chamado com identifier None ou vazio")
                return []
                
            conversation_id = None
            
            # Determinar se é phone ou conversation_id
            if identifier.startswith('conv_') or len(identifier) > 15:
                # Parece ser conversation_id
                conversation_id = identifier
                emoji_logger.system_info(f"Buscando mensagens por conversation_id: {conversation_id}")
            else:
                # Parece ser phone
                emoji_logger.system_info(f"Buscando mensagens por phone: {identifier}")
                conversation = await supabase_client.get_conversation_by_phone(identifier)
                if not conversation:
                    emoji_logger.system_warning(f"Conversa não encontrada para phone: {identifier}")
                    return []
                conversation_id = conversation["id"]
                emoji_logger.system_info(f"Conversation_id encontrado: {conversation_id}")
            
            # Buscar últimas 100 mensagens
            emoji_logger.system_info(f"Executando query para conversation_id: {conversation_id}")
            
            # GARANTIR que sempre tentamos buscar 100 mensagens
            query = supabase_client.client.table("messages")\
                .select("*")\
                .eq("conversation_id", conversation_id)\
                .order("created_at", desc=True)\
                .limit(100)
            
            response = query.execute()  # Removido await - cliente síncrono
            messages = response.data if response.data else []
            
            # Log detalhado para debug
            emoji_logger.system_info(f"📊 QUERY EXECUTADA:")
            emoji_logger.system_info(f"  • Conversation ID: {conversation_id}")
            emoji_logger.system_info(f"  • Mensagens encontradas: {len(messages)}")
            emoji_logger.system_info(f"  • Limite solicitado: 100")
            
            # Log das primeiras e últimas mensagens para debug
            if messages:
                first_msg = messages[0]
                last_msg = messages[-1]
                emoji_logger.system_info(f"  • Primeira msg: {first_msg.get('created_at', 'N/A')} - {first_msg.get('sender', 'N/A')}")
                emoji_logger.system_info(f"  • Última msg: {last_msg.get('created_at', 'N/A')} - {last_msg.get('sender', 'N/A')}")
            
            # Se encontrou menos de 100 mensagens, informar
            if len(messages) < 100:
                emoji_logger.system_warning(f"Apenas {len(messages)} mensagens disponíveis (menos que o limite de 100)")
            
            # Reverter para ordem cronológica
            messages.reverse()
            
            # Log de sucesso com informação completa
            success_msg = f"Mensagens recuperadas: {len(messages)}"
            if len(messages) < 100:
                success_msg += f" (conversa tem apenas {len(messages)} mensagens no total)"
            else:
                success_msg += " (limite de 100 atingido)"
                
            emoji_logger.supabase_success(success_msg, execution_time=0.1)
            
            # Retornar mensagens diretamente (sem cache)
            return messages
            
        except Exception as e:
            emoji_logger.supabase_error(f"Erro ao buscar mensagens: {e}",
                                       table="conversations")
            return []
    
    async def detect_emotional_triggers(
        self,
        messages: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Detecta gatilhos emocionais na conversa
        
        Args:
            messages: Histórico de mensagens
            
        Returns:
            Gatilhos emocionais detectados
        """
        # Verificar se análise emocional está habilitada
        if not self.emotional_triggers_enabled:
            return {
                "enabled": False,
                "message": "Análise emocional desabilitada"
            }
        
        triggers = {
            "frustration_indicators": 0,
            "excitement_indicators": 0,
            "hesitation_indicators": 0,
            "urgency_indicators": 0,
            "trust_indicators": 0
        }
        
        # Palavras-chave para cada emoção
        frustration_words = ["demora", "difícil", "complicado", "não entendo", "problema"]
        excitement_words = ["ótimo", "excelente", "adorei", "perfeito", "maravilha"]
        hesitation_words = ["não sei", "talvez", "preciso pensar", "dúvida", "será"]
        urgency_words = ["urgente", "rápido", "agora", "hoje", "imediato"]
        trust_words = ["confio", "acredito", "verdade", "sério", "garantia"]
        
        for msg in messages[-20:]:  # Últimas 20 mensagens
            content = msg.get("content", "").lower()
            
            for word in frustration_words:
                if word in content:
                    triggers["frustration_indicators"] += 1
            
            for word in excitement_words:
                if word in content:
                    triggers["excitement_indicators"] += 1
            
            for word in hesitation_words:
                if word in content:
                    triggers["hesitation_indicators"] += 1
            
            for word in urgency_words:
                if word in content:
                    triggers["urgency_indicators"] += 1
            
            for word in trust_words:
                if word in content:
                    triggers["trust_indicators"] += 1
        
        # Determinar estado emocional dominante
        max_trigger = max(triggers, key=triggers.get)
        triggers["dominant_emotion"] = max_trigger.replace("_indicators", "")
        
        return triggers
    
    async def should_call_sdr_team(
        self,
        context_analysis: Dict[str, Any],
        current_message: str
    ) -> Tuple[bool, Optional[str], str]:
        """
        Decide inteligentemente se deve chamar SDR Team
        
        Args:
            context_analysis: Análise contextual completa
            current_message: Mensagem atual
            
        Returns:
            (deve_chamar, agente_especifico, razão)
        """
        # Análise multi-fatorial para decisão
        decision_factors = {
            "complexity_score": 0,
            "specialization_needed": False,
            "confidence_level": 1.0,
            "recommended_agent": None,
            "reasoning": []
        }
        
        # Fator 1: Complexidade da solicitação - CALENDÁRIO
        # ✅ CORRIGIDO: Keywords específicas para evitar falsos positivos
        calendar_keywords = [
            "agendar reunião", "marcar reunião", "marcar encontro", "marcar meeting",
            "horário para reunião", "disponibilidade para", "agenda disponível",
            "calendário livre", "encontro para", "meeting para", "apresentação comercial",
            "reagendar", "remarcar reunião", "cancelar reunião",
            "que dia pode ser", "qual horário", "quando podemos nos reunir",
            "semana que vem para reunião", "próxima semana reunião", 
            "amanhã para reunião", "hoje para reunião", "vamos marcar"
        ]
        
        # ✅ NOVO: Filtro de saudação para evitar falsos positivos
        greeting_indicators = ["olá", "oi", "bom dia", "boa tarde", "boa noite", "tudo bem", "tchau", "obrigado"]
        is_simple_greeting = any(greeting in current_message.lower() for greeting in greeting_indicators) and len(current_message.split()) <= 5
        
        # ✅ CORRIGIDO: Indicadores negativos para agendamento
        negative_indicators = ["não", "nao", "sem interesse", "não quero", "já tenho", "não pedi"]
        has_negative_context = any(neg in current_message.lower() for neg in negative_indicators)
        
        # VERIFICAR SE É FOLLOW-UP/REENGAJAMENTO antes de detectar calendário
        followup_indicators = ["reengajamento", "follow-up", "não é agendamento", "parou de responder"]
        is_followup_message = any(indicator in current_message.lower() for indicator in followup_indicators)
        
        # ✅ CORRIGIDO: Lógica mais inteligente para detectar agendamento REAL
        calendar_detected = any(word in current_message.lower() for word in calendar_keywords)
        is_real_calendar_request = calendar_detected and not is_simple_greeting and not has_negative_context and not is_followup_message
        
        if is_real_calendar_request:
            # ✅ CORRIGIDO: Score mais conservador para evitar ativação desnecessária
            decision_factors["complexity_score"] += 0.6  # Reduzido de 0.8 para 0.6
            decision_factors["recommended_agent"] = "CalendarAgent"
            decision_factors["reasoning"].append("🗓️ Solicitação de agendamento detectada - Ativando CalendarAgent")
            
            # Log detalhado para debug
            logger.info(f"📅 CALENDÁRIO DETECTADO - Score: {decision_factors['complexity_score']}")
            logger.info(f"📅 Mensagem: {current_message[:100]}...")
            logger.info(f"📅 Agent recomendado: CalendarAgent")
        elif is_followup_message:
            # É uma mensagem de follow-up, não de agendamento
            decision_factors["reasoning"].append("🔄 Mensagem de follow-up detectada - evitando CalendarAgent")
            logger.info(f"🔄 FOLLOW-UP DETECTADO - Evitando CalendarAgent")
            logger.info(f"🔄 Mensagem: {current_message[:100]}...")
        
        # Fator 2: Análise de conta necessária
        if context_analysis.get("has_bill_image") or \
           "conta de luz" in current_message.lower():
            decision_factors["complexity_score"] += 0.5
            decision_factors["recommended_agent"] = "BillAnalyzerAgent"
            decision_factors["reasoning"].append("Análise de conta necessária")
        
        # Fator 3: Lead de alto valor
        qualification_signals = context_analysis.get("qualification_signals", {})
        if qualification_signals.get("bill_value", 0) > 4000:
            decision_factors["complexity_score"] += 0.3
            if not decision_factors["recommended_agent"]:
                decision_factors["recommended_agent"] = "QualificationAgent"
            decision_factors["reasoning"].append("Lead de alto valor detectado")
        
        # Fator 4: Múltiplas objeções
        if len(context_analysis.get("objections_raised", [])) > 2:
            decision_factors["complexity_score"] += 0.4
            decision_factors["reasoning"].append("Múltiplas objeções necessitam tratamento especializado")
        
        # Fator 5: Estágio avançado no funil
        if context_analysis.get("decision_stage") in ["negotiation", "closing"]:
            decision_factors["complexity_score"] += 0.3
            decision_factors["reasoning"].append("Estágio avançado requer expertise")
        
        # Fator 6: Necessidade de follow-up estratégico
        if context_analysis.get("conversation_duration", 0) > 24:  # horas
            decision_factors["complexity_score"] += 0.2
            if not decision_factors["recommended_agent"]:
                decision_factors["recommended_agent"] = "FollowUpAgent"
            decision_factors["reasoning"].append("Follow-up estratégico necessário")
        
        # Decisão final baseada em threshold inteligente
        should_call = decision_factors["complexity_score"] >= 0.7
        
        if should_call:
            reason = f"Score de complexidade: {decision_factors['complexity_score']:.2f}. " + \
                    ". ".join(decision_factors["reasoning"])
            
            emoji_logger.agentic_decision(f"Chamar SDR Team - {decision_factors['recommended_agent']}",
                                         score=decision_factors['complexity_score'],
                                         recommended_agent=decision_factors['recommended_agent'])
            return True, decision_factors["recommended_agent"], reason
        
        emoji_logger.agentic_decision("AGENTIC SDR resolve sozinha",
                                     score=decision_factors['complexity_score'])
        return False, None, "AGENTIC SDR pode resolver esta conversa"
    
    def _get_media_type_from_mimetype(self, mimetype: str) -> str:
        """
        Mapeia mimetype para tipo de mídia
        SIMPLES E FUNCIONAL!
        """
        if not mimetype:
            return "unknown"
        
        mimetype_lower = mimetype.lower()
        
        # Mapeamento simples e direto
        if "image" in mimetype_lower:
            return "image"
        elif "audio" in mimetype_lower:
            return "audio"
        elif "video" in mimetype_lower:
            return "video"
        elif "pdf" in mimetype_lower:
            return "pdf"
        elif mimetype_lower in ["application/vnd.openxmlformats-officedocument.wordprocessingml.document", "application/msword"]:
            return "document"
        elif "sticker" in mimetype_lower or "webp" in mimetype_lower:
            return "sticker"
        else:
            return "document"  # Default para documentos
    
    async def process_multimodal_content(
        self,
        media_type: str,
        media_data: str,
        caption: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Processa conteúdo multimodal (imagens, áudio, documentos)
        
        Args:
            media_type: Tipo de mídia (image, audio, document, pdf)
            media_data: Dados da mídia em base64
            caption: Legenda opcional
            
        Returns:
            Análise do conteúdo multimodal com estrutura padronizada
        """
        # Função helper para detectar formato
        def detect_and_clean_base64(data: str) -> tuple[str, str]:
            """
            Detecta e limpa dados base64
            Retorna: (base64_limpo, formato_detectado)
            """
            if not data:
                return "", "empty"
            
            # Se começa com data URL, extrair apenas o base64
            if data.startswith("data:"):
                if ";base64," in data:
                    clean_data = data.split(";base64,")[1]
                    return clean_data, "data_url"
                return "", "invalid_data_url"
            
            # Se é URL HTTP, não é base64
            if data.startswith(("http://", "https://")):
                return "", "url"
            
            # Verificar se é base64 válido
            if len(data) > 50:
                try:
                    # Tenta decodificar uma amostra
                    import base64
                    test = base64.b64decode(data[:100], validate=True)
                    return data, "base64"
                except:
                    # Pode ser texto ou outro formato
                    return "", "invalid"
            
            return "", "too_short"
        import time
        import asyncio
        start_time = time.time()
        
        # Configurar timeout de 30 segundos para todo o processamento
        MULTIMODAL_TIMEOUT = 30
        
        async def process_with_timeout():
            """Processa mídia com timeout"""
            # Usar nonlocal para acessar variáveis do escopo externo
            nonlocal media_data
            
            emoji_logger.system_info(f"🎯 MULTIMODAL: Iniciando processamento")
            emoji_logger.system_info(f"📌 Tipo: {media_type.upper()}")
            emoji_logger.system_info(f"📊 Tamanho dados base64: {len(media_data):,} caracteres")
            emoji_logger.system_info(f"💬 Caption: {caption[:50] + '...' if caption and len(caption) > 50 else caption or 'Sem legenda'}")
            emoji_logger.system_info(f"⏰ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            emoji_logger.system_info("═" * 50)
            
            # Verificar se análise multimodal está habilitada
            if not self.multimodal_enabled:
                emoji_logger.system_warning("Análise multimodal desabilitada nas configurações")
                return {
                    "type": media_type,
                    "enabled": False,
                    "message": "Análise multimodal desabilitada"
                }
            
            # Validar entrada
            if not media_data:
                emoji_logger.system_warning(f"❌ MULTIMODAL: Dados vazios para {media_type}")
                emoji_logger.system_warning(f"⏱️ Tempo decorrido: {time.time() - start_time:.2f}s")
                return {
                    "type": media_type,
                    "error": "Dados de mídia não fornecidos"
                }
            
            # Validar tipo de mídia
            valid_types = ["image", "audio", "document", "pdf", "video"]
            if media_type not in valid_types:
                emoji_logger.system_warning(f"❌ MULTIMODAL: Tipo inválido '{media_type}'")
                emoji_logger.system_warning(f"📝 Tipos válidos: {', '.join(valid_types)}")
                return {
                    "type": media_type,
                    "error": f"Tipo de mídia '{media_type}' não suportado"
                }
            if media_type == "image":
                # AGNO Framework - Processamento nativo de imagens
                emoji_logger.system_info("🌆 " + "=" * 45)
                emoji_logger.system_info("🌆 PROCESSAMENTO DE IMAGEM INICIADO")
                emoji_logger.system_info("🌆 " + "=" * 45)
                
                # Validar e limpar base64
                clean_base64, format_type = detect_and_clean_base64(media_data)
                
                emoji_logger.system_info(f"🔍 IMAGEM - Formato detectado: {format_type}")
                
                if format_type in ["empty", "invalid", "too_short", "url"]:
                    emoji_logger.system_warning(f"❌ IMAGEM: Formato inválido - {format_type}")
                    if format_type == "url":
                        emoji_logger.system_warning("💡 Dica: URL detectada, precisa baixar primeiro")
                    return {
                        "type": "image",
                        "error": f"Dados de imagem inválidos (formato: {format_type})",
                        "status": "invalid_format",
                        "format_detected": format_type
                    }
                
                # Usar o base64 limpo
                media_data = clean_base64
                
                if not media_data:
                    emoji_logger.system_warning("❌ IMAGEM: Dados vazios após limpeza")
                    return {
                        "type": "image",
                        "error": "Dados de imagem vazios",
                        "status": "no_data"
                    }
                
                # Verificar tamanho da imagem
                data_size = len(media_data)
                estimated_bytes = (data_size * 3) // 4  # Estimativa de bytes decodificados
                estimated_kb = estimated_bytes / 1024
                estimated_mb = estimated_kb / 1024
                
                emoji_logger.system_info(f"📈 IMAGEM - Métricas:")
                emoji_logger.system_info(f"  • Base64: {data_size:,} caracteres")
                emoji_logger.system_info(f"  • Estimado: {estimated_bytes:,} bytes ({estimated_kb:.1f} KB / {estimated_mb:.2f} MB)")
                
                # Validar qualidade da imagem ANTES de enviar
                try:
                    import base64 as b64_module
                    from PIL import Image
                    from io import BytesIO
                    
                    # Decodificar imagem
                    img_bytes = b64_module.b64decode(media_data)
                    img = Image.open(BytesIO(img_bytes))
                    width, height = img.size
                    
                    emoji_logger.system_info(f"📐 IMAGEM - Dimensões: {width}x{height} pixels")
                    
                    # Validar tamanho mínimo (100x100)
                    if width < 100 or height < 100:
                        emoji_logger.system_error("Image Validation", f"❌ IMAGEM: Muito pequena ({width}x{height}). Mínimo: 100x100")
                        return {
                            "type": "image",
                            "error": f"Imagem muito pequena ({width}x{height} pixels). Envie uma imagem maior que 100x100.",
                            "status": "too_small",
                            "dimensions": {"width": width, "height": height}
                        }
                    
                    # Validar tamanho máximo (10MB)
                    if len(img_bytes) > 10 * 1024 * 1024:
                        emoji_logger.system_error("Image Validation", f"❌ IMAGEM: Muito grande ({len(img_bytes) / 1024 / 1024:.1f}MB). Máximo: 10MB")
                        return {
                            "type": "image",
                            "error": "Imagem muito grande. Por favor, envie uma imagem menor que 10MB.",
                            "status": "too_large",
                            "size_mb": len(img_bytes) / 1024 / 1024
                        }
                    
                    # Avisos sobre qualidade
                    if data_size < 50000:  # Menos de 50KB em base64
                        emoji_logger.system_warning("⚠️ IMAGEM: Possível thumbnail detectada (<50KB)")
                    elif estimated_mb > 2:
                        emoji_logger.system_warning(f"⚠️ IMAGEM: Tamanho grande ({estimated_mb:.2f} MB) - pode causar lentidão")
                    
                except Exception as val_error:
                    emoji_logger.system_error("Image Validation", f"❌ Erro ao validar imagem: {str(val_error)}")
                    # Continuar mesmo com erro de validação
                
                # Preparar prompt específico para análise (simplificado para evitar erros)
                analysis_prompt = f"""Analise esta imagem e extraia as informações visíveis.
{f'Contexto: {caption}' if caption else ''}

Retorne em formato estruturado:
- Tipo de documento
- Valores encontrados
- Datas
- Nomes ou empresas
- Outras informações relevantes"""
                
                try:
                    # AGNO Framework Solution: Usar agno.media.Image nativo
                    import base64
                    import google.generativeai as genai
                    
                    emoji_logger.system_info("🔍 Etapa 1/4: Decodificando base64...")
                    
                    # AGNO Framework Solution: Usar agno.media.Image nativo
                    # Validar dados
                    if not media_data:
                        raise ValueError("Dados da imagem vazios")
                    
                    # Decodificar base64 para bytes
                    decode_start = time.time()
                    image_bytes = base64.b64decode(media_data)
                    original_size = len(image_bytes)
                    decode_time = time.time() - decode_start
                    
                    emoji_logger.system_info(f"✅ Decodificação completa em {decode_time:.2f}s")
                    emoji_logger.system_info(f"  • Tamanho real: {original_size:,} bytes")
                    emoji_logger.system_info(f"  • Taxa compressão: {(1 - original_size/data_size)*100:.1f}%")
                    
                    # AGNO Framework - Detecção robusta de formato de imagem
                    emoji_logger.system_info("🔍 Etapa 2/4: Detectando formato da imagem...")
                    detect_start = time.time()
                    detection_result = self.agno_media_detector.detect_media_type(image_bytes)
                    detect_time = time.time() - detect_start
                    
                    if not detection_result['detected']:
                        emoji_logger.system_warning(f"❌ IMAGEM: Formato não reconhecido")
                        emoji_logger.system_warning(f"  • Magic bytes: {detection_result.get('magic_bytes', 'N/A')}")
                        emoji_logger.system_warning(f"  • Tempo detecção: {detect_time:.2f}s")
                        # Usar fallback suggestion
                        fallback_msg = detection_result.get('fallback_suggestion', 'Formato não suportado')
                        return {
                            "type": "image",
                            "error": f"Formato não suportado: {fallback_msg}",
                            "status": "unsupported_format",
                            "agno_detection": detection_result
                        }
                    else:
                        emoji_logger.system_info(f"✅ Formato detectado: {detection_result['format'].upper()}")
                        emoji_logger.system_info(f"  • Confiança: {detection_result.get('confidence', 'N/A')}")
                        emoji_logger.system_info(f"  • Tempo detecção: {detect_time:.2f}s")
                    
                    format_hint = detection_result.get('format', 'unknown')
                    
                    # Verificar se recommended_params existe antes de acessar
                    if 'recommended_params' in detection_result:
                        agno_params = detection_result['recommended_params']
                    else:
                        # Usar parâmetros padrão se não houver recomendação
                        logger.warning(f"⚠️ Sem recommended_params para formato: {format_hint}")
                        agno_params = {
                            'format': 'auto',
                            'detail': 'high'
                        }
                    
                    emoji_logger.agentic_thinking(f"AGNO detectou: {format_hint} (confiança: {detection_result.get('confidence', 'unknown')})")
                    
                    # 🔧 CORREÇÃO MULTIMODAL: Usar PIL + Gemini diretamente (sem AGNO)
                    # Esta correção resolve o erro 400 INVALID_ARGUMENT e a latência de 42s
                    emoji_logger.system_info("🔧 Usando PIL + Gemini direto (correção implementada)")
                    
                    try:
                        # Processamento direto com PIL + Gemini (sem AGNO Framework)
                        from io import BytesIO
                        from PIL import Image as PILImage
                        import google.generativeai as genai
                        
                        # Decodificar base64 para usar com PIL
                        img_bytes = base64.b64decode(media_data)
                        img = PILImage.open(BytesIO(img_bytes))
                        
                        # Configurar Gemini com modelo Vision correto
                        from app.config import settings
                        genai.configure(api_key=settings.google_api_key)
                        vision_model = genai.GenerativeModel('gemini-1.5-flash')  # Modelo com capacidade vision
                        
                        # Prompt específico para análise de contas de energia
                        if "conta" in analysis_prompt.lower() or "energia" in analysis_prompt.lower():
                            enhanced_prompt = """Analise esta conta de energia elétrica e extraia as seguintes informações:
                            1. Valor total a pagar (em R$)
                            2. Consumo mensal em kWh
                            3. Nome completo do titular da conta
                            4. Endereço completo da instalação
                            5. Mês de referência da conta
                            6. Vencimento da fatura
                            
                            Responda em formato estruturado e claro. Se não conseguir identificar alguma informação, indique como "Não identificado"."""
                        else:
                            enhanced_prompt = analysis_prompt
                        
                        emoji_logger.system_info("📤 Enviando imagem para Gemini Vision com prompt otimizado...")
                        
                        # Enviar imagem com prompt otimizado ao Gemini
                        response = vision_model.generate_content([enhanced_prompt, img])
                        analysis_content = response.text if hasattr(response, 'text') else str(response)
                        
                        emoji_logger.system_info("✅ PIL + Gemini direto: Sucesso (latência otimizada)")
                            
                    except Exception as pil_gemini_error:
                        emoji_logger.system_error("PIL + Gemini direto falhou", f"Erro: {str(pil_gemini_error)}")
                        return {
                            "type": "image",
                            "error": f"Não foi possível processar a imagem: {str(pil_gemini_error)}",
                            "status": "error",
                            "suggestion": "Tente enviar a imagem em formato JPEG ou PNG"
                        }
                        
                    emoji_logger.agentic_multimodal("Análise de imagem concluída com sucesso")
                    
                    # Verificar se é conta de luz através da interpretação do Gemini
                    bill_keywords = ["conta", "energia", "kwh", "tarifa", "consumo", "fatura"]
                    is_bill = any(word in analysis_content.lower() for word in bill_keywords)
                    
                    if is_bill:
                        emoji_logger.agentic_multimodal("Conta de luz detectada", media_type="bill_image")
                        # Extrair valor da conta se possível
                        import re
                        
                        # Buscar padrão de valor monetário na análise
                        valor_match = re.search(r'R\$\s*(\d+[.,]\d{2})', analysis_content)
                        bill_amount = None
                        if valor_match:
                            # Converter vírgula para ponto e para float
                            bill_amount = float(valor_match.group(1).replace(',', '.'))
                            emoji_logger.system_info(f"💰 Valor da conta detectado: R$ {bill_amount:.2f}")
                        
                        return {
                            "type": "bill_image",
                            "needs_analysis": True,
                            "content": analysis_content,
                            "bill_amount": bill_amount,  # Adicionar valor extraído
                            "is_bill": True  # Garantir que é reconhecido como conta
                        }
                    else:
                        # Imagem genérica
                        return {
                            "type": "image",
                            "content": analysis_content,
                            "caption": caption,
                            "processed": True
                        }
                    
                except Exception as img_error:
                    emoji_logger.system_error("Vision API", f"Erro ao analisar imagem: {str(img_error)[:100]}")
                    
                    # Tentar extrair informações do erro para diagnóstico
                    error_details = str(img_error)
                    if "quota" in error_details.lower():
                        error_msg = "Limite de API excedido"
                    elif "invalid" in error_details.lower():
                        error_msg = "Formato de imagem inválido"
                    elif "timeout" in error_details.lower():
                        error_msg = "Timeout na análise"
                    else:
                        error_msg = f"Erro na análise: {str(img_error)[:100]}"
                    
                    return {
                        "type": "image",
                        "error": error_msg,
                        "status": "error",
                        "is_thumbnail": data_size < 50000
                    }
            
            elif media_type == "audio":
                # Processar áudio (transcrição)
                emoji_logger.system_info("Processamento de áudio solicitado")
                
                # Verificar se transcrição está habilitada
                if not self.settings.enable_voice_message_transcription:
                    return {
                        "type": "audio",
                        "status": "disabled",
                        "message": "Transcrição de áudio desabilitada"
                    }
                
                # Usar o novo AudioTranscriber
                try:
                    from app.services.audio_transcriber import audio_transcriber
                    
                    emoji_logger.agentic_thinking("Transcrevendo áudio com AudioTranscriber...")
                    
                    # Detectar mimetype do áudio (geralmente audio/ogg no WhatsApp)
                    mimetype = "audio/ogg"  # Padrão do WhatsApp
                    
                    # Transcrever
                    result = await audio_transcriber.transcribe_from_base64(
                        media_data,
                        mimetype=mimetype,
                        language="pt-BR"
                    )
                    
                    if result["status"] == "success":
                        # SIMPLES: Apenas retornar a transcrição
                        # Não precisa processar com agente adicional
                        transcribed_text = result["text"]
                        
                        emoji_logger.agentic_multimodal(
                            f"Audio transcrito com sucesso: {len(transcribed_text)} caracteres",
                            media_type="audio",
                            duration=result.get("duration", 0)
                        )
                        
                        return {
                            "type": "audio",
                            "transcription": transcribed_text,
                            "duration": result.get("duration", 0),
                            "engine": result.get("engine", "Google Speech Recognition"),
                            "status": "transcribed"
                        }
                    elif result["status"] == "unclear":
                        return {
                            "type": "audio",
                            "status": "unclear",
                            "message": "Não foi possível compreender o áudio claramente",
                            "transcription": result.get("text", "")
                        }
                    else:
                        emoji_logger.system_warning(f"❌ ÁUDIO: Erro na transcrição")
                        emoji_logger.system_warning(f"  • Erro: {result.get('error', 'Erro desconhecido')}")
                        emoji_logger.system_warning(f"  • Tempo total: {time.time() - start_time:.2f}s")
                        return {
                            "type": "audio",
                            "status": "error",
                            "message": f"Erro na transcrição: {result.get('error', 'Erro desconhecido')}"
                        }
                        
                except Exception as e:
                    emoji_logger.system_warning(f"Erro ao transcrever áudio: {e}")
                    return {
                        "type": "audio",
                        "status": "error",
                        "message": f"Erro ao processar áudio: {str(e)}"
                    }
            
            elif media_type in ["document", "pdf"]:
                # AGNO Framework - Processamento nativo de documentos
                emoji_logger.agentic_multimodal("Processando documento com AGNO Framework nativo")
                
                try:
                    # AGNO Framework Solution: Usar document readers nativos
                    import base64
                    from io import BytesIO
                    
                    # Decodificar base64 para bytes
                    document_bytes = base64.b64decode(media_data)
                    original_size = len(document_bytes)
                    emoji_logger.agentic_multimodal(f"Documento decodificado: {original_size:,} bytes")
                    
                    # AGNO Framework - Detecção robusta de formato de documento
                    detection_result = self.agno_media_detector.detect_media_type(document_bytes)
                    
                    if not detection_result['detected']:
                        emoji_logger.system_warning(f"Formato de documento não reconhecido pelo AGNO: {detection_result.get('magic_bytes', 'N/A')}")
                        # Usar fallback suggestion
                        fallback_msg = detection_result.get('fallback_suggestion', 'Formato não suportado')
                        return {
                            "type": "document",
                            "error": f"Formato não suportado: {fallback_msg}",
                            "status": "unsupported_format",
                            "agno_detection": detection_result
                        }
                    
                    document_type = detection_result.get('format', 'unknown')
                    
                    # Verificar se recommended_params existe antes de acessar
                    if 'recommended_params' in detection_result:
                        agno_params = detection_result['recommended_params']
                    else:
                        # Usar parâmetros padrão se não houver recomendação
                        logger.warning(f"⚠️ Sem recommended_params para documento: {document_type}")
                        agno_params = {
                            'reader_class': 'PDFReader',
                            'ocr_enabled': True,
                            'max_pages': None
                        }
                    
                    is_pdf = document_type == 'pdf'
                    is_docx = document_type == 'docx'
                    
                    emoji_logger.agentic_thinking(f"AGNO detectou documento: {document_type} (confiança: {detection_result.get('confidence', 'unknown')})")
                    
                    # Determinar tipo e usar AGNO reader apropriado
                    extracted_text = ""
                    doc_metadata = {}
                    
                    if is_pdf or is_docx:
                        try:
                            # Usar processador centralizado de documentos
                            emoji_logger.agentic_thinking(f"Processando documento {document_type} com EnhancedDocumentProcessor...")
                            
                            # Processar documento usando função SIMPLES (substitui document_processor_enhanced.py)
                            result = await self._process_document_simple(
                                data=document_bytes,
                                filename=f"document.{document_type}"
                            )
                            
                            if result.get('status') == 'success':
                                extracted_text = result.get('content', '')
                                doc_metadata = {
                                    "reader": "enhanced_document_processor",
                                    "format": document_type,
                                    "size_bytes": original_size,
                                    "text_extracted": result.get('text_extracted', False),
                                    "images_processed": result.get('images_processed', 0),
                                    "ocr_content": result.get('ocr_content', False)
                                }
                                
                                if is_pdf:
                                    doc_metadata["pages"] = result.get('pages', 0)
                                elif is_docx:
                                    doc_metadata["sections"] = result.get('sections', 0)
                                
                                emoji_logger.agentic_thinking(f"Documento processado com sucesso: {len(extracted_text)} caracteres extraídos")
                            else:
                                raise Exception(f"Falha no processamento: {result.get('error', 'Unknown error')}")
                            
                        except Exception as doc_error:
                            emoji_logger.system_error("Document Processing", f"Erro ao processar documento: {str(doc_error)}")
                            # Não lançar exceção, retornar erro estruturado
                            return {
                                "type": "document",
                                "error": str(doc_error),
                                "status": "error",
                                "message": f"Erro ao processar documento: {str(doc_error)[:100]}"
                            }
                    
                    else:
                        # Formato não suportado pelos readers AGNO
                        raise Exception(f"Formato de documento não suportado. Magic bytes: {magic_bytes[:8].hex()}")
                    
                    # Processar resultado
                    result = {
                        "status": "success",
                        "text": extracted_text,
                        "document_type": document_type,
                        "metadata": doc_metadata
                    }
                    
                    if result["status"] == "success":
                        extracted_text = result["text"]
                        doc_type = result.get("document_type", "documento")
                        
                        # Criar contexto para análise do documento
                        doc_context = f"""O cliente enviou um {doc_type} com o seguinte conteúdo:
                        
                        {extracted_text[:3000]}...
                        
                        Documento completo: {result.get('pages', 'N/A')} página(s)
                        Tipo identificado: {doc_type}
                        
                        Por favor, analise o documento e:
                        1. Identifique as informações principais
                        2. Se for uma conta de luz, extraia valor e consumo
                        3. Se for outro documento, resuma os pontos importantes"""
                        
                        # Usar IntelligentModelFallback diretamente para evitar dependência do OpenAI
                        emoji_logger.agentic_thinking("Analisando documento com IntelligentModelFallback...")
                        try:
                            # Criar um agente temporário apenas com o modelo inteligente
                            from agno.agent import Agent as AgnoAgent
                            
                            temp_agent = AgnoAgent(
                                model=self.intelligent_model,  # Usa o wrapper com fallback
                                markdown=True,
                                show_tool_calls=False,
                                instructions="Você é um assistente especializado em análise de documentos. Extraia todas as informações relevantes de forma detalhada."
                            )
                            
                            # Processar documento
                            response = temp_agent.run(doc_context)
                            
                                                # Extrair conteúdo da resposta - CORREÇÃO DEFINITIVA AGNO RunResponse
                    raw_response = None
                    
                    # 1. Primeiro tentar content (mais comum)
                    if hasattr(result, 'content') and result.content:
                        raw_response = result.content
                        emoji_logger.system_info(f"✅ Conteúdo extraído de result.content")
                    
                    # 2. Se content vazio, verificar messages (AGNO RunResponse padrão)
                    elif hasattr(result, 'messages') and result.messages:
                        emoji_logger.system_info(f"📬 result.content vazio, verificando messages ({len(result.messages)} mensagens)")
                        # Procurar última mensagem do assistant
                        for msg in reversed(result.messages):
                            if hasattr(msg, 'role') and msg.role == 'assistant':
                                if hasattr(msg, 'content') and msg.content:
                                    raw_response = msg.content
                                    emoji_logger.system_info(f"✅ Conteúdo extraído de messages[assistant]")
                                    break
                                elif hasattr(msg, 'text') and msg.text:
                                    raw_response = msg.text
                                    emoji_logger.system_info(f"✅ Conteúdo extraído de messages[assistant].text")
                                    break
                    
                    # 3. Verificar outros atributos comuns
                    if not raw_response:
                        if hasattr(result, 'text') and result.text:
                            raw_response = result.text
                            emoji_logger.system_info(f"✅ Conteúdo extraído de result.text")
                        elif hasattr(result, 'message') and result.message:
                            raw_response = result.message
                            emoji_logger.system_info(f"✅ Conteúdo extraído de result.message")
                        elif hasattr(result, 'reasoning_content') and result.reasoning_content:
                            raw_response = result.reasoning_content
                            emoji_logger.system_info(f"✅ Conteúdo extraído de result.reasoning_content")
                    
                    # 4. Se ainda vazio, tentar dict
                    if not raw_response and isinstance(result, dict):
                        raw_response = result.get('content') or result.get('text') or result.get('message')
                        if raw_response:
                            emoji_logger.system_info(f"✅ Conteúdo extraído de result dict")
                    
                    # 5. Debug completo se ainda vazio
                    if not raw_response:
                        emoji_logger.system_error("Response Extraction", f"Falha total na extração!")
                        emoji_logger.system_error("Response Extraction", f"Tipo: {type(result)}")
                        emoji_logger.system_error("Response Extraction", f"Atributos: {[attr for attr in dir(result) if not attr.startswith('_')]}")
                        if hasattr(result, 'messages') and result.messages:
                            emoji_logger.system_error("Response Extraction", f"Messages[0]: {result.messages[0] if result.messages else 'vazio'}")
                        if hasattr(result, 'status'):
                            emoji_logger.system_error("Response Extraction", f"Status: {result.status}")
                        
                        # Último recurso - converter para string
                        raw_response = str(result)
                    
                    # Debug: Log do conteúdo extraído
                    emoji_logger.system_info(f"📄 raw_response (primeiros 200 chars): {raw_response[:200] if raw_response else 'VAZIO'}...")
                    emoji_logger.system_info(f"📏 Tamanho raw_response: {len(raw_response) if raw_response else 0} caracteres")
                    
                    # Verificar se resposta está vazia antes de processar
                    if not raw_response or str(raw_response).strip() == "":
                        emoji_logger.system_warning("⚠️ raw_response está vazio ANTES da verificação!")
                    
                    # ✅ CORREÇÃO: Verificar se a resposta está vazia antes de processar
                    if not raw_response or raw_response.strip() == "":
                        emoji_logger.system_warning("⚠️ Agent retornou resposta vazia! Usando fallback...")
                        
                        # Debug: Entender por que está vazio
                        if multimodal_result and 'content' in multimodal_result and not multimodal_result.get('error'):
                            emoji_logger.system_info("🔍 Tinha análise multimodal mas agente retornou vazio")
                            emoji_logger.system_info(f"🔍 Análise multimodal: {multimodal_result.get('content', '')[:100]}...")
                            
                            # Fallback especializado para quando há análise multimodal
                            if multimodal_result.get('is_bill'):
                                # É uma conta de luz
                                bill_amount = multimodal_result.get('bill_amount', 0)
                                if bill_amount > 0:
                                    raw_response = f"Perfeito! Vi aqui sua conta de luz no valor de R$ {bill_amount:.2f}. Com esse valor, consigo fazer uma análise bem precisa de quanto você pode economizar com energia solar! Esse valor está pesando no orçamento?"
                                else:
                                    raw_response = "Ótimo! Recebi a foto da sua conta de luz. Para fazer uma análise precisa da economia, você pode me dizer qual o valor médio que está pagando?"
                            elif multimodal_result.get('type') == 'image':
                                # Imagem genérica
                                raw_response = "Legal! Recebi sua imagem. Para eu fazer uma proposta personalizada de economia com energia solar, me conta: qual o valor médio da sua conta de luz?"
                            elif multimodal_result.get('type') == 'audio':
                                # Áudio transcrito
                                transcription = multimodal_result.get('transcription', '')
                                if transcription:
                                    raw_response = f"Entendi! Ouvi seu áudio dizendo: '{transcription[:100]}...'. Como posso ajudar você com energia solar?"
                                else:
                                    raw_response = "Recebi seu áudio! Para fazer uma análise completa, preciso saber: qual o valor da sua conta de luz?"
                            else:
                                # Fallback genérico com mídia
                                raw_response = "Obrigada por enviar! Para fazer uma proposta personalizada de economia, preciso saber o valor da sua conta de luz. Quanto você paga em média?"
                        else:
                            emoji_logger.system_info("🔍 Sem análise multimodal disponível")
                            # Fallback com base no estágio atual
                            current_stage = lead_data.get('current_stage', 'INITIAL_CONTACT') if lead_data else 'INITIAL_CONTACT'
                            if current_stage == 'INITIAL_CONTACT':
                                raw_response = "Oi! Tudo bem? Sou a Helen da SolarPrime! Antes de começarmos, como posso te chamar?"
                            else:
                                raw_response = "Oi! Desculpe, tive um probleminha aqui. Pode repetir sua última mensagem?"
                    
                    # ✅ CORREÇÃO: Verificar se já há tags antes de adicionar (evita duplicação)
                    if "<RESPOSTA_FINAL>" in raw_response:
                        # Resposta já tem tags - usar diretamente
                        response = raw_response
                        emoji_logger.system_debug("✅ Tags <RESPOSTA_FINAL> já presentes - usando resposta diretamente")
                    else:
                        # Resposta sem tags - adicionar tags para extração
                        response = f"<RESPOSTA_FINAL>{raw_response}</RESPOSTA_FINAL>"
                        emoji_logger.system_debug("➕ Adicionando tags <RESPOSTA_FINAL> à resposta")
                    
                    # 🚨 VALIDAÇÃO DE SEGURANÇA: Verificar se está pedindo dados proibidos
                    forbidden_terms = [
                        'cpf', 'c.p.f', 'cadastro de pessoa', 'documento',
                        'rg', 'r.g', 'identidade', 'cnh', 'c.n.h',
                        'carteira de motorista', 'carteira de identidade',
                        'dados bancários', 'conta bancária', 'senha',
                        'cartão de crédito', 'dados do cartão'
                    ]
                    
                    response_lower = response.lower()
                    
                    # CORREÇÃO: Usar regex para detectar palavras completas, não substrings
                    import re
                    contains_forbidden = False
                    for term in forbidden_terms:
                        # \b marca limites de palavra para evitar falsos positivos
                        pattern = r'\b' + re.escape(term) + r'\b'
                        if re.search(pattern, response_lower):
                            contains_forbidden = True
                            break
                    
                    if contains_forbidden:
                        emoji_logger.system_warning("🚨 ALERTA: Resposta contém solicitação de dados proibidos!")
                        emoji_logger.system_warning(f"Resposta original: {response}")
                        
                        # Substituir resposta por uma segura baseada no contexto
                        if multimodal_result and 'content' in multimodal_result:
                            # Se tem análise de imagem, focar nisso
                            analysis = multimodal_result.get('content', '')
                            if 'conta' in analysis.lower() and 'valor' in analysis.lower():
                                response = "<RESPOSTA_FINAL>Perfeito! Vi sua conta de luz aqui. Vamos calcular quanto você pode economizar com energia solar! Me conta, esse valor está pesando no seu bolso?</RESPOSTA_FINAL>"
                            else:
                                response = "<RESPOSTA_FINAL>Legal! Recebi sua imagem. Para fazer uma análise completa, preciso saber: qual o valor médio da sua conta de luz?</RESPOSTA_FINAL>"
                        else:
                            # Resposta genérica segura
                            response = "<RESPOSTA_FINAL>Ótimo! Para eu fazer uma proposta personalizada de economia, preciso apenas saber o valor da sua conta de luz. Quanto você está pagando em média?</RESPOSTA_FINAL>"
                        
                        emoji_logger.system_debug(f"✅ Resposta substituída por versão segura: {response}")
                    
                except Exception as agent_error:
                    emoji_logger.system_error("AGENTIC SDR", f"Erro ao gerar resposta: {agent_error}")
                    # Fallback para resposta padrão
                    response = None
            
            # Garantir que SEMPRE temos uma resposta
            if not response or response.strip() == "":
                emoji_logger.system_warning("Nenhuma resposta gerada, usando fallback")
                # Resposta fallback baseada no contexto
                if "oi" in message.lower() or "olá" in message.lower() or "ola" in message.lower():
                    response = "<RESPOSTA_FINAL>Oi! Tudo bem? Sou a Helen da Solar Prime! Como posso ajudar você hoje?</RESPOSTA_FINAL>"
                elif "bom dia" in message.lower():
                    response = "<RESPOSTA_FINAL>Bom dia! Que legal você entrar em contato! Sou a Helen da Solar Prime. Em que posso ajudar?</RESPOSTA_FINAL>"
                elif "boa tarde" in message.lower():
                    response = "<RESPOSTA_FINAL>Boa tarde! Obrigada por entrar em contato com a Solar Prime! Sou a Helen, como posso ajudar?</RESPOSTA_FINAL>"
                elif "boa noite" in message.lower():
                    response = "<RESPOSTA_FINAL>Boa noite! Que bom falar com você! Sou a Helen da Solar Prime. Como posso ajudar?</RESPOSTA_FINAL>"
                else:
                    response = "<RESPOSTA_FINAL>Olá! Sou a Helen da Solar Prime. Vi sua mensagem e adoraria ajudar! Você tem interesse em economizar na conta de luz com energia solar?</RESPOSTA_FINAL>"
            
            # 7. Atualizar estado emocional da Helen com análise completa
            try:
                # Recalcular com dados completos da conversa
                current_state = current_emotional_state or "ENTUSIASMADA"
                new_emotional_state = self._update_emotional_state(
                    emotional_triggers, 
                    context_analysis,
                    current_state
                )
                
                # Salva o novo estado no banco para a próxima interação (usando import global)
                if conversation_id:
                    await supabase_client.update_conversation_emotional_state(
                        conversation_id,
                        new_emotional_state
                    )
            except Exception as e:
                emoji_logger.system_error("AGENTIC SDR", f"Erro ao atualizar estado emocional: {str(e)}")
                new_emotional_state = current_emotional_state or "ENTUSIASMADA"
            
            # 8. Memória é gerenciada automaticamente pelo Agent no AGNO v1.7.6
            # O Agent salva automaticamente as interações quando configurado com memory
            # Não precisa chamar explicitamente memory.add()
            
            # 9. Aplicar simulação de digitação natural
            # Garantir que response tem um valor antes de aplicar simulação
            if response:
                response = self._apply_typing_simulation(response)
            else:
                # Fallback final se ainda não houver resposta
                response = "<RESPOSTA_FINAL>Oi! 😊 Sou a Helen da Solar Prime. Como posso ajudar você hoje?</RESPOSTA_FINAL>"
            
            # 10. Determinar se deve reagir ou responder citando
            result = {
                "text": response,
                "reaction": None,
                "reply_to": None
            }
            
            # Lógica mais natural: apenas ~10% de chance de reagir ou citar
            import random
            
            # Reações: apenas para mensagens muito específicas (10% de chance)
            message_lower = message.lower().strip()
            if random.random() < 0.1:  # 10% de chance
                # Reações para confirmações muito curtas
                if len(message_lower) < 10 and any(word in message_lower for word in ["ok", "blz", "👍"]):
                    result["reaction"] = "👍"
                # Reações para agradecimentos explícitos
                elif len(message_lower) < 20 and any(word in message_lower for word in ["obrigado", "obrigada", "valeu"]):
                    result["reaction"] = "❤️"
                # Reações para risadas
                elif any(indicator in message_lower for indicator in ["kkkkk", "hahaha", "😂😂", "🤣🤣"]):
                    result["reaction"] = "😂"
            
            # Reação especial para imagens/documentos recebidos
            if media and media.get("type") in ["image", "document", "pdf"]:
                result["reaction"] = "✅"  # Confirma recebimento de mídia
            
            # Citações: apenas em contextos muito específicos (10% de chance)
            if message_id and random.random() < 0.1:
                # Citar quando há múltiplas perguntas ou contexto importante
                question_count = message.count("?")
                if question_count > 1:  # Múltiplas perguntas
                    result["reply_to"] = message_id
                # Citar quando está respondendo a uma dúvida específica após outras mensagens
                elif conversation_id and len(messages_history or []) > 5 and "?" in message:
                    result["reply_to"] = message_id
            
            emoji_logger.agentic_response(f"Resposta gerada: {response[:100]}...")
            
            # Retornar estrutura enriquecida
            return result
            
        except Exception as e:
            emoji_logger.system_error("AGENTIC SDR", f"Erro crítico ao processar: {e}")
            # Resposta de emergência mais natural
            emergency_responses = [
                "<RESPOSTA_FINAL>Oi! Sou a Helen da Solar Prime! Como posso ajudar você hoje com energia solar?</RESPOSTA_FINAL>",
                "<RESPOSTA_FINAL>Olá! Que bom você entrar em contato! Sou a Helen, especialista em energia solar. Em que posso ajudar?</RESPOSTA_FINAL>",
                "<RESPOSTA_FINAL>Oi! Tudo bem? Sou a Helen da Solar Prime! Você tem interesse em economizar na conta de luz?</RESPOSTA_FINAL>"
            ]
            import random
            # Retornar estrutura consistente mesmo em erro
            return {
                "text": random.choice(emergency_responses),
                "reaction": None,
                "reply_to": None
            }
    
    async def _personalize_team_response(
        self,
        team_response: str,
        emotional_triggers: Dict[str, Any],
        emotional_state: str = "ENTUSIASMADA"
    ) -> str:
        """Personaliza resposta do Team com toque do AGENTIC SDR"""
        
        # Adicionar personalização baseada no estado emocional
        personalization_prompt = f"""
        Resposta técnica: {team_response}
        
        Emoção do lead: {emotional_triggers.get('dominant_emotion')}
        Seu estado emocional: {emotional_state}
        
        Reescreva mantendo a informação mas com seu toque pessoal,
        empatia e naturalidade. Mantenha breve e direto.
        """
        
        # Em AGNO v1.7.6, usar run()
        # Usar arun() para suporte assíncrono com timeout
        PERSONALIZATION_TIMEOUT = 15  # timeout menor para personalização
        
        try:
            if hasattr(self.agent, 'arun'):
                result = await asyncio.wait_for(
                    self.agent.arun(personalization_prompt),
                    timeout=PERSONALIZATION_TIMEOUT
                )
            else:
                # Fallback para run() se arun() não estiver disponível
                result = await asyncio.wait_for(
                    self.agent.run(personalization_prompt),
                    timeout=PERSONALIZATION_TIMEOUT
                )
        except asyncio.TimeoutError:
            emoji_logger.system_warning(f"Timeout na personalização após {PERSONALIZATION_TIMEOUT}s, usando resposta original")
            return response  # Retorna resposta original sem personalização
        except Exception as e:
            emoji_logger.system_error("Personalization", f"Erro na personalização: {str(e)}, usando resposta original")
            return response  # Retorna resposta original sem personalização
        
        # Extrair conteúdo da resposta com múltiplas tentativas
        if hasattr(result, 'content') and result.content is not None:
            raw_response = result.content
        elif hasattr(result, 'text') and result.text is not None:
            raw_response = result.text
        elif hasattr(result, 'message') and result.message is not None:
            raw_response = result.message
        elif isinstance(result, dict):
            raw_response = result.get('content') or result.get('text') or result.get('message') or str(result)
        else:
            raw_response = str(result)
        
        # ✅ CORREÇÃO: Verificar se já há tags antes de adicionar (evita duplicação)
        if "<RESPOSTA_FINAL>" in raw_response:
            # Resposta já tem tags - usar diretamente
            emoji_logger.system_debug("✅ Tags <RESPOSTA_FINAL> já presentes na personalização - usando diretamente")
            return raw_response
        else:
            # Resposta sem tags - adicionar tags para extração
            emoji_logger.system_debug("➕ Adicionando tags <RESPOSTA_FINAL> à personalização")
            return f"<RESPOSTA_FINAL>{raw_response}</RESPOSTA_FINAL>"
    
    def _update_emotional_state(
        self,
        emotional_triggers: Dict[str, Any],
        context_analysis: Dict[str, Any],
        current_state: str
    ) -> str:
        """Calcula novo estado emocional baseado na conversa - Alinhado com banco"""
        
        # Validar estado atual
        valid_states = [state.value for state in EmotionalState]
        if current_state not in valid_states:
            current_state = EmotionalState.NEUTRA.value
        
        # Lógica de transição de estados atualizada
        dominant_emotion = emotional_triggers.get("dominant_emotion")
        
        if dominant_emotion == "frustration" or dominant_emotion == "hesitation":
            # Usuário com dúvidas ou hesitação
            new_state = EmotionalState.DUVIDOSA.value
        
        elif dominant_emotion == "excitement" or dominant_emotion == "interest":
            # Usuário animado ou interessado
            new_state = EmotionalState.ENTUSIASMADA.value
        
        elif dominant_emotion == "curiosity" or emotional_triggers.get("questions_asked", 0) > 2:
            # Usuário fazendo muitas perguntas
            new_state = EmotionalState.CURIOSA.value
        
        elif context_analysis.get("decision_stage") == "decision" or \
             context_analysis.get("conversion_probability", 0) > 0.7:
            # Usuário próximo da decisão
            new_state = EmotionalState.CONFIANTE.value
        
        elif emotional_triggers.get("neutral_indicators", 0) > 2:
            # Conversa neutra/inicial
            new_state = EmotionalState.NEUTRA.value
        
        else:
            # Mantém o estado atual se válido
            new_state = current_state if current_state in valid_states else EmotionalState.NEUTRA.value
        
        emoji_logger.agentic_thinking(f"Estado emocional atualizado: {new_state}",
                                     emotional_state=new_state)
        
        return new_state
    
    def _apply_typing_simulation(self, text: str) -> str:
        """Retorna o texto sem modificação - typing é feito via Evolution API"""
        # IMPORTANTE: Esta função NÃO deve modificar o texto!
        # O indicador "digitando..." é enviado corretamente via Evolution API em webhooks.py
        # Qualquer quebra de linha aqui causa problemas no WhatsApp
        return text
    
    def get_metrics(self) -> Dict[str, Any]:
        """Retorna métricas do agente"""
        return {
            "emotional_state": self.emotional_state.value,
            "cognitive_load": self.cognitive_load,
            "is_initialized": self.is_initialized
        }


# Factory function - SEMPRE cria nova instância para isolamento total
async def create_agentic_sdr() -> AgenticSDR:
    """Cria e inicializa nova instância do AGENTIC SDR para cada requisição"""
    agent = AgenticSDR()
    await agent.initialize()
    return agent